"""
Data Preprocessing Script (Optional)
scripts/preprocess_data.py

This is mainly for processing external datasets like RESCO.
For synthetic SUMO data, this step can be skipped.
"""

import argparse
import shutil
from pathlib import Path
import json

def main():
    parser = argparse.ArgumentParser(
        description='Preprocess traffic data (optional for SUMO synthetic data)'
    )
    parser.add_argument('--input', type=str, required=True, help='Input directory')
    parser.add_argument('--output', type=str, required=True, help='Output directory')
    parser.add_argument('--validation-split', type=float, default=0.2, help='Validation split ratio')
    
    args = parser.parse_args()
    
    input_dir = Path(args.input)
    output_dir = Path(args.output)
    
    print("="*60)
    print("Data Preprocessing")
    print("="*60)
    
    # Check if input directory exists and has data
    if not input_dir.exists():
        print(f"\n⚠️  Input directory not found: {input_dir}")
        print("\nFor synthetic SUMO data, preprocessing is not required!")
        print("Your generated traffic scenarios are ready to use.")
        print("\nYou can proceed directly to training:")
        print("  python scripts/train.py --config config/train_config.yaml \\")
        print("      --network single_intersection --algorithm DQN --episodes 1000 --gpu 0")
        return
    
    # Check for RESCO or other external datasets
    resco_dir = input_dir / 'resco'
    if resco_dir.exists():
        print(f"\nFound RESCO dataset in {resco_dir}")
        print("Processing RESCO data...")
        # Add RESCO-specific preprocessing here if needed
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Copy and organize
        for scenario_dir in resco_dir.iterdir():
            if scenario_dir.is_dir():
                dest = output_dir / scenario_dir.name
                dest.mkdir(parents=True, exist_ok=True)
                print(f"  Processing: {scenario_dir.name}")
        
        print("✓ RESCO data preprocessed")
    else:
        print(f"\n⚠️  No external datasets found in {input_dir}")
        print("\nFor synthetic SUMO data generated by generate_traffic.py:")
        print("  → No preprocessing needed")
        print("  → Data is ready to use directly")
    
    print("\n" + "="*60)
    print("Preprocessing Complete")
    print("="*60)
    
    # Create metadata
    metadata = {
        'input_dir': str(input_dir),
        'output_dir': str(output_dir),
        'validation_split': args.validation_split,
        'processed': True
    }
    
    output_dir.mkdir(parents=True, exist_ok=True)
    with open(output_dir / 'metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"\nMetadata saved to: {output_dir / 'metadata.json'}")

if __name__ == '__main__':
    main()